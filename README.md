<img width="889" alt="Screenshot 2022-08-03 053248" src="https://user-images.githubusercontent.com/103318089/182519890-af727ccd-b7cf-4988-b346-c3f82d51501e.png">

# A Bias Framework Comparison: FairDetect - Aequitas
> 

## Table of Contents
* [General Info](#general-information)
* [Technologies Used](#technologies-used)
* [Features](#features)
* [Acknowledgements](#acknowledgements)
* [Contact](#contact)


## General Information
- The project's focus is on investigating any potential bias by contrasting two frameworks, Fairdetect and AEquitas. This project's main goal is to 
spot potential biases by using the Synthetic Credit Card and Modcloth databases as examples.
- To further enhance use, performance, and reliability, we have implemented additional modifications to the original FairDetect framework inside the 
confines of this project.
- Finally, a second bias framework has been presented in order to compare various bias detection approach elements such as functionality and utility. In order to complete this exercise, we used AEquitas, a framework that was created by the Centre for Data Science and Public Policy at the University of Chicago. 
Users can easily audit models for various bias and fairness indicators in connection to numerous population sub-groups using this open-source tool. 

## Technologies Used
- Fair Detect framework
- Fair Detect groupb framework version - 0.47
- Aequitas framework 


## Features
- Whenever a rejection of the null hypothesis happens and bias is detected, the statement is printed out in bold letters to show
- Added Docstrings understanding the functionality of the larger part of the code
- Introduced Disparate Impact Ratio in the class
- Introduced user experience improvements that helps users select critical features for their bias detection and ML exercise which saves them the hassle of encoding the same features
- Introduced Quality control method that checks if Target value selected


## Acknowledgements
- This project was based on the thesis autored by Ryan Daher: "Transparent unfairness: An Approach to Investigating Machine Learning Bias".


## Contact
- Carlos Blazquez - https://github.com/CarlosBlazquezP
- Christian Barba - https://github.com/CBRodulfo
- Diana Fernandez - https://github.com/dianisley
- Dominik Roser - https://github.com/domro11
- Hiba Shanaa - https://github.com/hibashanaa
- Mark Hourany - https://github.com/markantoinehourany

